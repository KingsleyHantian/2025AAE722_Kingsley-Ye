{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from ISLP import load_data\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: Load and Examine Default Dataset\n",
        "\n",
        "Load the Default dataset from the ISLP library and examine its structure. Report the dataset dimensions, the column names and their data types, and the distribution of the default variable (how many Yes vs. No). Then fit a logistic regression model to predict default using income, balance, and student as predictors (using the entire dataset). Report the coefficient for balance and interpret its meaning in terms of the log-odds of defaulting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Default dataset\n",
        "Default = load_data('Default')\n",
        "\n",
        "# Examine dataset structure\n",
        "print(\"Dataset dimensions:\", Default.shape)\n",
        "print(\"\\nColumn names and data types:\")\n",
        "print(Default.dtypes)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(Default.head())\n",
        "\n",
        "# Distribution of default variable\n",
        "print(\"\\nDistribution of default variable:\")\n",
        "default_counts = Default['default'].value_counts()\n",
        "print(default_counts)\n",
        "print(f\"\\nProportion of defaults: {default_counts['Yes'] / len(Default):.3f}\")\n",
        "print(f\"Proportion of non-defaults: {default_counts['No'] / len(Default):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit logistic regression model using entire dataset\n",
        "# Prepare features and target\n",
        "X = Default[['income', 'balance', 'student']].copy()\n",
        "# Convert student to binary (Yes=1, No=0)\n",
        "X['student'] = (X['student'] == 'Yes').astype(int)\n",
        "y = (Default['default'] == 'Yes').astype(int)\n",
        "\n",
        "# Fit logistic regression\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': ['income', 'balance', 'student'],\n",
        "    'Coefficient': log_reg.coef_[0]\n",
        "})\n",
        "print(\"Logistic Regression Coefficients:\")\n",
        "print(coefficients)\n",
        "\n",
        "# Report balance coefficient specifically\n",
        "balance_coef = log_reg.coef_[0][1]  # balance is the second feature\n",
        "print(f\"\\nBalance coefficient: {balance_coef:.6f}\")\n",
        "print(f\"\\nInterpretation: For every $1 increase in balance, the log-odds of defaulting increases by {balance_coef:.6f}.\")\n",
        "print(f\"This means that higher credit card balances are associated with higher probability of defaulting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2: Train/Test Split and LDA/QDA Models\n",
        "\n",
        "Split the Default dataset into training (70%) and testing (30%) sets using train_test_split with random_state=42. Fit both Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) models using income and balance as predictors. For the LDA model, report the class means for each predictor and the prior probabilities for each class. Generate predictions on the test set for both LDA and QDA, create confusion matrices, and report their test accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for LDA/QDA (only income and balance)\n",
        "X_features = Default[['income', 'balance']].copy()\n",
        "y_labels = Default['default']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"Training set default distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test set default distribution: {y_test.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit LDA model\n",
        "lda = LDA()\n",
        "lda.fit(X_train, y_train)\n",
        "\n",
        "# Report LDA class means and prior probabilities\n",
        "print(\"LDA Model Results:\")\n",
        "print(f\"Classes: {lda.classes_}\")\n",
        "print(f\"\\nPrior probabilities:\")\n",
        "for i, class_name in enumerate(lda.classes_):\n",
        "    print(f\"  {class_name}: {lda.priors_[i]:.4f}\")\n",
        "\n",
        "print(f\"\\nClass means for each predictor:\")\n",
        "means_df = pd.DataFrame(lda.means_, \n",
        "                       columns=['income', 'balance'],\n",
        "                       index=lda.classes_)\n",
        "print(means_df)\n",
        "\n",
        "# Make predictions\n",
        "lda_pred = lda.predict(X_test)\n",
        "lda_accuracy = accuracy_score(y_test, lda_pred)\n",
        "\n",
        "print(f\"\\nLDA Test Accuracy: {lda_accuracy:.4f}\")\n",
        "print(\"\\nLDA Confusion Matrix:\")\n",
        "lda_cm = confusion_matrix(y_test, lda_pred)\n",
        "print(lda_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit QDA model\n",
        "qda = QDA()\n",
        "qda.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "qda_pred = qda.predict(X_test)\n",
        "qda_accuracy = accuracy_score(y_test, qda_pred)\n",
        "\n",
        "print(f\"QDA Test Accuracy: {qda_accuracy:.4f}\")\n",
        "print(\"\\nQDA Confusion Matrix:\")\n",
        "qda_cm = confusion_matrix(y_test, qda_pred)\n",
        "print(qda_cm)\n",
        "\n",
        "# Compare LDA and QDA accuracies\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"LDA Accuracy: {lda_accuracy:.4f}\")\n",
        "print(f\"QDA Accuracy: {qda_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3: Naive Bayes Classifier\n",
        "\n",
        "Using the same train/test split from Question 2, fit a Naive Bayes classifier (GaussianNB) with income and balance as predictors. Generate predictions on the test set and create a confusion matrix. Compare the test accuracy of Naive Bayes with the LDA and QDA results. Finally, use the predict_proba() method to find the predicted probability of default for a customer with income = 40000 and balance = 2000.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "nb_pred = nb.predict(X_test)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "\n",
        "print(f\"Naive Bayes Test Accuracy: {nb_accuracy:.4f}\")\n",
        "print(\"\\nNaive Bayes Confusion Matrix:\")\n",
        "nb_cm = confusion_matrix(y_test, nb_pred)\n",
        "print(nb_cm)\n",
        "\n",
        "# Compare with LDA and QDA\n",
        "print(f\"\\nAccuracy Comparison:\")\n",
        "print(f\"LDA Accuracy: {lda_accuracy:.4f}\")\n",
        "print(f\"QDA Accuracy: {qda_accuracy:.4f}\")\n",
        "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict probability for specific customer\n",
        "customer_data = np.array([[40000, 2000]])  # income=40000, balance=2000\n",
        "customer_proba = nb.predict_proba(customer_data)\n",
        "\n",
        "print(f\"Predicted probabilities for customer with income=40000, balance=2000:\")\n",
        "print(f\"Probability of No (non-default): {customer_proba[0][0]:.4f}\")\n",
        "print(f\"Probability of Yes (default): {customer_proba[0][1]:.4f}\")\n",
        "print(f\"\\nPredicted class: {nb.predict(customer_data)[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4: K-Nearest Neighbors (KNN)\n",
        "\n",
        "Using the same train/test split, apply feature scaling to income and balance with StandardScaler. Fit K-Nearest Neighbors (KNN) models with n_neighbors = 1, 3, 5, and 10, and evaluate their test performance. Create a table summarizing the test accuracy for each K value. Identify which K gives the best performance and explain why very small values of K (such as K=1) may not be optimal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling applied successfully.\")\n",
        "print(f\"Scaled training set mean: {X_train_scaled.mean(axis=0)}\")\n",
        "print(f\"Scaled training set std: {X_train_scaled.std(axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit KNN models with different K values\n",
        "k_values = [1, 3, 5, 10]\n",
        "knn_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    knn_pred = knn.predict(X_test_scaled)\n",
        "    knn_accuracy = accuracy_score(y_test, knn_pred)\n",
        "    \n",
        "    knn_results.append({\n",
        "        'K': k,\n",
        "        'Accuracy': knn_accuracy,\n",
        "        'Confusion_Matrix': confusion_matrix(y_test, knn_pred)\n",
        "    })\n",
        "    \n",
        "    print(f\"K={k}: Test Accuracy = {knn_accuracy:.4f}\")\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, knn_pred))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "knn_summary = pd.DataFrame([\n",
        "    {'K': result['K'], 'Test_Accuracy': result['Accuracy']}\n",
        "    for result in knn_results\n",
        "])\n",
        "\n",
        "print(\"KNN Performance Summary:\")\n",
        "print(knn_summary.to_string(index=False))\n",
        "\n",
        "# Find best K\n",
        "best_k_result = max(knn_results, key=lambda x: x['Accuracy'])\n",
        "best_k = best_k_result['K']\n",
        "best_accuracy = best_k_result['Accuracy']\n",
        "\n",
        "print(f\"\\nBest K value: {best_k} with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nExplanation for why K=1 may not be optimal:\")\n",
        "print(f\"- K=1 is very sensitive to noise and outliers\")\n",
        "print(f\"- It can lead to overfitting, especially with small datasets\")\n",
        "print(f\"- The decision boundary becomes very irregular and complex\")\n",
        "print(f\"- Higher K values provide smoother decision boundaries and better generalization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5: Comprehensive Comparison and Analysis\n",
        "\n",
        "Create a summary table comparing the test accuracy of all methods implemented: Logistic Regression (refit on training data), LDA, QDA, Naive Bayes, and the best KNN from Question 4. Using the confusion matrices, identify which method has the lowest false negative rate. If the cost of missing a default is 10 times higher than a false alarm, recommend which method should be used and explain why. Finally, for your chosen method, adjust the probability threshold from 0.5 to 0.3 and report how this change affects the false positive and false negative rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refit logistic regression on training data only\n",
        "X_train_full = Default[['income', 'balance', 'student']].iloc[X_train.index].copy()\n",
        "X_train_full['student'] = (X_train_full['student'] == 'Yes').astype(int)\n",
        "y_train_binary = (y_train == 'Yes').astype(int)\n",
        "\n",
        "X_test_full = Default[['income', 'balance', 'student']].iloc[X_test.index].copy()\n",
        "X_test_full['student'] = (X_test_full['student'] == 'Yes').astype(int)\n",
        "y_test_binary = (y_test == 'Yes').astype(int)\n",
        "\n",
        "log_reg_train = LogisticRegression(random_state=42)\n",
        "log_reg_train.fit(X_train_full, y_train_binary)\n",
        "log_reg_pred = log_reg_train.predict(X_test_full)\n",
        "log_reg_accuracy = accuracy_score(y_test_binary, log_reg_pred)\n",
        "\n",
        "print(f\"Logistic Regression (training data only) Test Accuracy: {log_reg_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison table\n",
        "comparison_results = [\n",
        "    {'Method': 'Logistic Regression', 'Test_Accuracy': log_reg_accuracy},\n",
        "    {'Method': 'LDA', 'Test_Accuracy': lda_accuracy},\n",
        "    {'Method': 'QDA', 'Test_Accuracy': qda_accuracy},\n",
        "    {'Method': 'Naive Bayes', 'Test_Accuracy': nb_accuracy},\n",
        "    {'Method': f'KNN (K={best_k})', 'Test_Accuracy': best_accuracy}\n",
        "]\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "comparison_df = comparison_df.sort_values('Test_Accuracy', ascending=False)\n",
        "\n",
        "print(\"Comprehensive Method Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best method\n",
        "best_method = comparison_df.iloc[0]\n",
        "print(f\"\\nBest performing method: {best_method['Method']} with accuracy: {best_method['Test_Accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate false negative rates from confusion matrices\n",
        "def calculate_fnr(cm):\n",
        "    # cm is [[TN, FP], [FN, TP]]\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    return fnr\n",
        "\n",
        "def calculate_fpr(cm):\n",
        "    # cm is [[TN, FP], [FN, TP]]\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    return fpr\n",
        "\n",
        "# Get confusion matrices for all methods\n",
        "log_reg_cm = confusion_matrix(y_test_binary, log_reg_pred)\n",
        "lda_cm_binary = confusion_matrix((y_test == 'Yes').astype(int), (lda_pred == 'Yes').astype(int))\n",
        "qda_cm_binary = confusion_matrix((y_test == 'Yes').astype(int), (qda_pred == 'Yes').astype(int))\n",
        "nb_cm_binary = confusion_matrix((y_test == 'Yes').astype(int), (nb_pred == 'Yes').astype(int))\n",
        "best_knn_cm_binary = confusion_matrix((y_test == 'Yes').astype(int), \n",
        "                                     (KNeighborsClassifier(n_neighbors=best_k).fit(X_train_scaled, y_train).predict(X_test_scaled) == 'Yes').astype(int))\n",
        "\n",
        "print(\"False Negative Rates:\")\n",
        "print(f\"Logistic Regression: {calculate_fnr(log_reg_cm):.4f}\")\n",
        "print(f\"LDA: {calculate_fnr(lda_cm_binary):.4f}\")\n",
        "print(f\"QDA: {calculate_fnr(qda_cm_binary):.4f}\")\n",
        "print(f\"Naive Bayes: {calculate_fnr(nb_cm_binary):.4f}\")\n",
        "print(f\"KNN (K={best_k}): {calculate_fnr(best_knn_cm_binary):.4f}\")\n",
        "\n",
        "# Find method with lowest FNR\n",
        "fnr_results = [\n",
        "    ('Logistic Regression', calculate_fnr(log_reg_cm)),\n",
        "    ('LDA', calculate_fnr(lda_cm_binary)),\n",
        "    ('QDA', calculate_fnr(qda_cm_binary)),\n",
        "    ('Naive Bayes', calculate_fnr(nb_cm_binary)),\n",
        "    (f'KNN (K={best_k})', calculate_fnr(best_knn_cm_binary))\n",
        "]\n",
        "\n",
        "lowest_fnr_method = min(fnr_results, key=lambda x: x[1])\n",
        "print(f\"\\nMethod with lowest False Negative Rate: {lowest_fnr_method[0]} (FNR = {lowest_fnr_method[1]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost analysis: missing a default costs 10x more than false alarm\n",
        "print(\"Cost Analysis (Missing default = 10x cost of false alarm):\")\n",
        "print(\"\\nFor each method, calculating total cost:\")\n",
        "\n",
        "cost_results = []\n",
        "for method_name, cm in [('Logistic Regression', log_reg_cm),\n",
        "                        ('LDA', lda_cm_binary),\n",
        "                        ('QDA', qda_cm_binary),\n",
        "                        ('Naive Bayes', nb_cm_binary),\n",
        "                        (f'KNN (K={best_k})', best_knn_cm_binary)]:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    # Cost: FP = 1, FN = 10\n",
        "    total_cost = fp * 1 + fn * 10\n",
        "    cost_results.append((method_name, total_cost, fp, fn))\n",
        "    print(f\"{method_name}: Total Cost = {total_cost} (FP={fp}, FN={fn})\")\n",
        "\n",
        "# Find method with lowest cost\n",
        "best_cost_method = min(cost_results, key=lambda x: x[1])\n",
        "print(f\"\\nRecommended method based on cost analysis: {best_cost_method[0]}\")\n",
        "print(f\"Total cost: {best_cost_method[1]} (FP={best_cost_method[2]}, FN={best_cost_method[3]})\")\n",
        "print(f\"\\nExplanation: This method minimizes the total cost where missing a default (FN) costs 10 times more than a false alarm (FP).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjust probability threshold for the recommended method\n",
        "recommended_method = best_cost_method[0]\n",
        "\n",
        "if 'Logistic Regression' in recommended_method:\n",
        "    # Get probabilities for logistic regression\n",
        "    proba_05 = log_reg_train.predict_proba(X_test_full)[:, 1]\n",
        "    pred_05 = (proba_05 > 0.5).astype(int)\n",
        "    pred_03 = (proba_05 > 0.3).astype(int)\n",
        "    \n",
        "    cm_05 = confusion_matrix(y_test_binary, pred_05)\n",
        "    cm_03 = confusion_matrix(y_test_binary, pred_03)\n",
        "    \n",
        "elif 'LDA' in recommended_method:\n",
        "    proba_05 = lda.predict_proba(X_test)[:, 1]\n",
        "    pred_05 = (proba_05 > 0.5).astype(int)\n",
        "    pred_03 = (proba_05 > 0.3).astype(int)\n",
        "    \n",
        "    cm_05 = confusion_matrix((y_test == 'Yes').astype(int), pred_05)\n",
        "    cm_03 = confusion_matrix((y_test == 'Yes').astype(int), pred_03)\n",
        "    \n",
        "elif 'QDA' in recommended_method:\n",
        "    proba_05 = qda.predict_proba(X_test)[:, 1]\n",
        "    pred_05 = (proba_05 > 0.5).astype(int)\n",
        "    pred_03 = (proba_05 > 0.3).astype(int)\n",
        "    \n",
        "    cm_05 = confusion_matrix((y_test == 'Yes').astype(int), pred_05)\n",
        "    cm_03 = confusion_matrix((y_test == 'Yes').astype(int), pred_03)\n",
        "    \n",
        "elif 'Naive Bayes' in recommended_method:\n",
        "    proba_05 = nb.predict_proba(X_test)[:, 1]\n",
        "    pred_05 = (proba_05 > 0.5).astype(int)\n",
        "    pred_03 = (proba_05 > 0.3).astype(int)\n",
        "    \n",
        "    cm_05 = confusion_matrix((y_test == 'Yes').astype(int), pred_05)\n",
        "    cm_03 = confusion_matrix((y_test == 'Yes').astype(int), pred_03)\n",
        "    \n",
        "else:  # KNN\n",
        "    knn_best = KNeighborsClassifier(n_neighbors=best_k)\n",
        "    knn_best.fit(X_train_scaled, y_train)\n",
        "    proba_05 = knn_best.predict_proba(X_test_scaled)[:, 1]\n",
        "    pred_05 = (proba_05 > 0.5).astype(int)\n",
        "    pred_03 = (proba_05 > 0.3).astype(int)\n",
        "    \n",
        "    cm_05 = confusion_matrix((y_test == 'Yes').astype(int), pred_05)\n",
        "    cm_03 = confusion_matrix((y_test == 'Yes').astype(int), pred_03)\n",
        "\n",
        "print(f\"Threshold Analysis for {recommended_method}:\")\n",
        "print(f\"\\nThreshold = 0.5:\")\n",
        "print(f\"Confusion Matrix: {cm_05.ravel()}\")\n",
        "print(f\"False Positive Rate: {calculate_fpr(cm_05):.4f}\")\n",
        "print(f\"False Negative Rate: {calculate_fnr(cm_05):.4f}\")\n",
        "\n",
        "print(f\"\\nThreshold = 0.3:\")\n",
        "print(f\"Confusion Matrix: {cm_03.ravel()}\")\n",
        "print(f\"False Positive Rate: {calculate_fpr(cm_03):.4f}\")\n",
        "print(f\"False Negative Rate: {calculate_fnr(cm_03):.4f}\")\n",
        "\n",
        "print(f\"\\nChange in FPR: {calculate_fpr(cm_03) - calculate_fpr(cm_05):.4f}\")\n",
        "print(f\"Change in FNR: {calculate_fnr(cm_03) - calculate_fnr(cm_05):.4f}\")\n",
        "print(f\"\\nInterpretation: Lowering the threshold from 0.5 to 0.3 makes the model more sensitive to predicting defaults,\")\n",
        "print(f\"which reduces false negatives but increases false positives.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "\n",
        "1. **Logistic Regression**: Provided interpretable coefficients showing the relationship between predictors and log-odds of default\n",
        "2. **LDA and QDA**: Linear and quadratic discriminant analysis methods with different assumptions about class distributions\n",
        "3. **Naive Bayes**: Simple probabilistic classifier assuming feature independence\n",
        "4. **KNN**: Non-parametric method requiring feature scaling, with performance varying by K value\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
